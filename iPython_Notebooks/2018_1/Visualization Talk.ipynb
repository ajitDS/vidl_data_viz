{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing Stack Overflow Data in Python\n",
    "\n",
    "In this notebook, we visualize posts on Stack Overflow from September 2017. The data was compiled from searches on the [Stack Exchange Data Explorer](https://data.stackexchange.com/stackoverflow/query/new). Location information was added using the [Google Maps API](https://developers.google.com/maps/).\n",
    "\n",
    "<div id=\"contents\"></div>\n",
    "## Table of Contents\n",
    "1. [Load the Data](#load)\n",
    "1. [Visualize Completeness](#completeness)\n",
    "1. [Visualize Time](#time)\n",
    "1. [Visualize Tags](#tags)\n",
    "1. [Explore Text](#explore)\n",
    "1. [Plot Place](#place)\n",
    "1. [Plot Connections](#network)\n",
    "1. [Conclusion](#conclusion)\n",
    "\n",
    "Make sure to go through the first two sections (data and completeness) first. The other sections can be done out of order.\n",
    "\n",
    "## Load Libraries\n",
    "This cell contains all the libraries which are necessary for this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General purpose libraries\n",
    "# A nice library for reading in csv data\n",
    "import pandas as pd\n",
    "# A library which most visualization libraries in Python are built on.\n",
    "# We will start by using it to make some plots with pandas\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "# A library for doing math\n",
    "import numpy as np\n",
    "# A library for turning unicode fields into ASCII fields\n",
    "import unicodedata\n",
    "# a regex library\n",
    "import re\n",
    "# a class which makes counting the number of times something occurs in a list easier\n",
    "from collections import Counter\n",
    "\n",
    "# some functions for displaying html in a notebook\n",
    "from IPython.core.display import display, HTML\n",
    "\n",
    "# A library to visualize holes in a dataset\n",
    "import missingno as msno\n",
    "# A library to make wordclouds\n",
    "import wordcloud\n",
    "\n",
    "# Libraries for Word Trees\n",
    "# lets us use graphviz in python\n",
    "from pydotplus import graphviz\n",
    "# to display the final Image\n",
    "from IPython.display import Image\n",
    "\n",
    "# Libraries interactive charts\n",
    "from bokeh.io import output_notebook\n",
    "# display interactive charts inline\n",
    "output_notebook()\n",
    "from bokeh.palettes import Viridis6 as palette\n",
    "from bokeh.plotting import figure, show\n",
    "from bokeh.models import HoverTool, ColorBar, LinearColorMapper, FixedTicker, ColumnDataSource, LogColorMapper\n",
    "# to make patches into glyphs and treat counties and states differently\n",
    "from bokeh.models.glyphs import Patches\n",
    "\n",
    "# shape files for US counties\n",
    "from bokeh.sampledata.us_counties import data as counties\n",
    "# shape files for US states\n",
    "from bokeh.sampledata import us_states as us_states_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"load\"></div>\n",
    "## Load the Data\n",
    "*[Table of Contents](#contents)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data\n",
    "posts = pd.read_csv('SeptemberPosts.csv')\n",
    "posts.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Columns\n",
    "What do all these columns mean?\n",
    "- PostId = the id in Stack Overflow's database of this post\n",
    "- Score = the score given to the post by people voting up and down on it\n",
    "- PostType = What type of post is this?\n",
    "- CreationData = When was this post posted?\n",
    "- Title = The text in the title of the post\n",
    "- UserId = The id of the user who posted in the Stack Overflow database\n",
    "- Reputation = The reputaiton of the user who posted\n",
    "- Location = The location the user put down as their home on their profile\n",
    "- Tags = Tags which are associated with this post\n",
    "- QuestionId = The question this post is linked to\n",
    "\n",
    "Let us convert CreationDate to a datetime type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts['CreationDate'] = pd.to_datetime(posts['CreationDate'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"completeness\"></div>\n",
    "## Visualizing Completeness\n",
    "*[Table of Contents](#contents)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'd like to know how complete our data is, so let's look at which fields have null values for the answers and questions using [missingno](https://github.com/ResidentMario/missingno)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msno.matrix(posts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is there any correlation between variables as to when they are blank?\n",
    "\n",
    "Missingno has a lot of ways of visualizing data completeness. One of them is to see the correlation in missing-ness between fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msno.heatmap(posts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that posts with a title or tags never have a question id. I wonder if these groups make up different types of posts. Let's investigate which post types we have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_type_counts = posts['PostType'].value_counts()\n",
    "post_type_counts.plot(kind='bar', color='DarkBlue')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is there a way to make this chart interactive?\n",
    "\n",
    "Let's use [Bokeh](https://bokeh.pydata.org/en/latest/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOOLS = \"pan,wheel_zoom,reset,hover,save\"\n",
    "\n",
    "p = figure(\n",
    "    title=\"Post Types\",\n",
    "    tools=TOOLS,\n",
    "    x_range=post_type_counts.index.tolist(),\n",
    "    plot_height=400\n",
    ")\n",
    "p.vbar(x=post_type_counts.index.values, top=post_type_counts.values, width=0.9)\n",
    "\n",
    "hover = p.select_one(HoverTool)\n",
    "hover.point_policy = \"follow_mouse\"\n",
    "hover.tooltips = [(\"Number of Posts\", \"@top\")]\n",
    "\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most posts are either questions or answers. These types of posts serve very different purposes, so let's seperate them out and see how complete each is.\n",
    "\n",
    "This time we'll try a different method of visualizing missing data in which we count up how often each attribute is not missing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = posts[posts['PostType'] == 'Question']\n",
    "answers = posts[posts['PostType'] == 'Answer']\n",
    "\n",
    "print(\"Questions\")\n",
    "msno.bar(questions)\n",
    "print(\"Answers\")\n",
    "msno.bar(answers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usually, when you fing out that you having missing data you want to know why and what is going on with those points. Fortunately Pandas makes this very easy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts[posts['UserId'].isnull()].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to investigate these posts further, we can see posts and answers with null `UserId`s using Jupyter's HTML capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_link(p):\n",
    "    if p['PostType'] == 'Answer':\n",
    "        link = '\"https://stackoverflow.com/questions/{0}#answer-{1}\"'.format(int(p['QuestionId']), int(p['PostId']))\n",
    "        return '<a href='+link+' target=\"_blank\">Answer without user</a>'\n",
    "    else:\n",
    "        link = '\"https://stackoverflow.com/questions/{0}\"'.format(int(p['PostId']))\n",
    "        return '<a href='+link+' target=\"_blank\">Question without user</a>'\n",
    "\n",
    "display(HTML('<br/>'.join(posts[posts['UserId'].isnull()].head().apply(lambda p: get_link(p), axis=1))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"time\"></div>\n",
    "## Visualizing Time\n",
    "*[Table of Contents](#contents)*\n",
    "\n",
    "1. What time of day do people post?\n",
    "1. [How long did it take to get an answer in September?](#firstReply)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts['CreationDate'].apply(lambda x: x.hour).hist(bins=range(24))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"firstReply\"></div>\n",
    "When was the first reply for each question?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aggregate answers by question id\n",
    "answers_by_question = answers.groupby('QuestionId')['CreationDate'].agg(min)\n",
    "# get the earliest creation date for each answer\n",
    "first_reply = pd.DataFrame({'PostId':answers_by_question.index.values, 'EarliestReply':answers_by_question.values})\n",
    "# add the time of the earliest answer to the questions data frame (filtering out questions which were not answered)\n",
    "first_reply = pd.merge(first_reply, questions, how='inner', on=['PostId'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the time it took to get an answer\n",
    "gap = (first_reply['EarliestReply']-first_reply['CreationDate'])\n",
    "# convert to minutes\n",
    "gap /= pd.Timedelta(minutes=1)\n",
    "\n",
    "# find the median\n",
    "print('Median answer time for questions asked and answered in September 2017 is {0} min.'.format(gap.median()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot\n",
    "plt.hist(gap.tolist(), bins = 50)\n",
    "#plt.yscale('log')\n",
    "plt.ylabel('Number of Questions')\n",
    "plt.xlabel('Time in Minutes')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"tags\"></div>\n",
    "## Visualize Tags\n",
    "*[Table of Contents](#contents)*\n",
    "\n",
    "What do we ask about when we ask about data and errors in programing?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_wordcloud(series, title):\n",
    "    # stitch all the text together\n",
    "    text = ' '.join(series.tolist())\n",
    "    # make a wordcloud from the text\n",
    "    title_wordcloud = wordcloud.WordCloud().generate(text)\n",
    "    # we want the words in our cloud to all be the same color\n",
    "    title_wordcloud.recolor(color_func=lambda word, **kwargs:'white')\n",
    "    # turn the wordcloud into an image\n",
    "    plt.imshow(title_wordcloud, interpolation='bilinear')\n",
    "    # we don't want an x and y axis\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(title + ' (' + str(len(series)) + ' questions)')\n",
    "    plt.show()\n",
    "\n",
    "def get_tags(words):\n",
    "    words = ['[^\\w]{}[^\\w]'.format(w) for w in words]\n",
    "    tags = questions[questions['Title'].str.lower().str.contains('|'.join(words))]['Tags']\n",
    "    tags = tags.apply(lambda x: x.strip('<>').split('><'))\n",
    "    return tags\n",
    "\n",
    "data_tags = get_tags(['data', 'scrape', 'clean', 'open', 'load'])\n",
    "text_to_wordcloud(data_tags.apply(lambda x: ' '.join(x)), 'Data Tags')\n",
    "error_tags = get_tags(['error', 'wrong', 'throws', 'throw', 'why'])\n",
    "text_to_wordcloud(error_tags.apply(lambda x: ' '.join(x)), 'Error Tags')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is really just turning words in numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tag_count(series):\n",
    "    return pd.Series([t for tag_list in series for t in tag_list]).value_counts()\n",
    "\n",
    "tag_counts = pd.concat([get_tag_count(data_tags), get_tag_count(error_tags)], axis=1)\n",
    "tag_counts = tag_counts.rename(columns={0:'data', 1:'error'})\n",
    "tag_counts = tag_counts[tag_counts['data'].notnull() & tag_counts['error'].notnull()]\n",
    "\n",
    "x = tag_counts['data'].values\n",
    "y = tag_counts['error'].values\n",
    "TOOLS = \"pan,wheel_zoom,reset,hover,save\"\n",
    "\n",
    "source = ColumnDataSource(data=dict(\n",
    "    x=x,\n",
    "    y=y,\n",
    "    data_perc=[round(val/len(data_tags)*100,2) for val in x],\n",
    "    error_perc=[round(val/len(error_tags)*100,2) for val in y],\n",
    "    name=tag_counts.index.tolist()\n",
    "))\n",
    "\n",
    "# uncomment the lines to switch this to a log scale to see the number for less popular languages\n",
    "p = figure(\n",
    "    title=\"Tags\",\n",
    "    tools=TOOLS,\n",
    "#     x_axis_type=\"log\",\n",
    "#     y_axis_type=\"log\",\n",
    "    plot_height=400\n",
    ")\n",
    "p.circle('x', 'y', size=10, source=source)\n",
    "p.xaxis.axis_label = \"Data Posts\"\n",
    "p.yaxis.axis_label = \"Error Posts\"\n",
    "\n",
    "hover = p.select_one(HoverTool)\n",
    "hover.point_policy = \"follow_mouse\"\n",
    "hover.tooltips = [(\"Tag\", \"@name\"), (\"% of data posts:\", \"@data_perc\"), (\"% of error posts:\", \"@error_perc\")]\n",
    "\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"explore\"></div>\n",
    "## Text Visualization\n",
    "*[Table of Contents](#contents)*\n",
    "\n",
    "What do users post about when they post a question tagged 'python'? The Body column has a lot of HTML tags in it's values, which I don't feel like deeling with right now, so let's look at what question askers put in the titles of their posts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "python_titles = questions[questions['Tags'].str.contains('python')]['Title'].str.lower()\n",
    "text_to_wordcloud(python_titles, 'Python Titles')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other Methods of Looking at Word Frequencies\n",
    "\n",
    "One problem with word clouds is that they divorce the words in them from any context. This isn't much of a problem when the words are tags, which aren't part of a sentence. But when looking at titles it would be nice to see what users are saying when they use the words 'python', 'using' and 'file'.\n",
    "\n",
    "To really understand all 19,462 questions tagged 'python', we'd have to break out some machine learning methods, but their are ways visualize context better than a word cloud. One of them is called a [Word Tree](http://hint.fm/papers/wordtree_final2.pdf) and was developed by the many eyes group at IBM. You can see [examples](https://www.jasondavies.com/wordtree/) of this style of visualization made in d3.js.\n",
    "\n",
    "Since Jupyter notebooks can embed HTML elements, word trees rendered in d3 can be embedded in a notebook. However, for this tutorial we are sticking to python, so I will demonstrate how to build a word tree using a library which runs [graphviz](http://graphviz.org) in python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a variable to help us mark nodes as distinct when they have the same label\n",
    "node_counter = 0\n",
    "\n",
    "# a class to keep track of a node and it's connections\n",
    "class Node:\n",
    "    def __init__(self, word, count, matching_strings, graph, reverse=False, branching=3, highlight=False):\n",
    "        global node_counter\n",
    "        if highlight:\n",
    "            self.node = graphviz.Node(node_counter, label=word+'\\n'+str(count), peripheries=2, fontsize=20)\n",
    "        else:\n",
    "            self.node = graphviz.Node(node_counter, label=word+'\\n'+str(count))\n",
    "        node_counter += 1\n",
    "        graph.add_node(self.node)\n",
    "        if count > 1:\n",
    "            self.generate_children(matching_strings, graph, reverse, branching)\n",
    "    \n",
    "    def generate_children(self, matching_strings, graph, reverse, branching):\n",
    "        if len(matching_strings) == 0:\n",
    "            return\n",
    "        matching_strings = matching_strings[matching_strings.apply(len) > 0]\n",
    "        all_children = Counter(matching_strings.apply(lambda x:x[-1 if reverse else 0]))\n",
    "        children = all_children.most_common(branching)\n",
    "        for word, count in children:\n",
    "            if not reverse:\n",
    "                child_matches = matching_strings[matching_strings.apply(lambda x:x[0]) == word].apply(lambda x:x[1:])\n",
    "                c_node = Node(word, count, child_matches, graph=graph, reverse=reverse, branching=branching)\n",
    "                graph.add_edge(graphviz.Edge(self.node, c_node.node))\n",
    "            else:\n",
    "                child_matches = matching_strings[matching_strings.apply(lambda x:x[-1]) == word].apply(lambda x:x[:-1])\n",
    "                c_node = Node(word, count, child_matches, graph=graph, reverse=reverse, branching=branching)\n",
    "                graph.add_edge(graphviz.Edge(c_node.node, self.node))\n",
    "        left_over = sum(all_children.values()) - sum([x[1] for x in children])\n",
    "        if left_over > 0:\n",
    "            c_node = Node('...', left_over, [], graph=graph, reverse=reverse, branching=branching)\n",
    "            if reverse:\n",
    "                graph.add_edge(graphviz.Edge(c_node.node, self.node))\n",
    "            else:\n",
    "                graph.add_edge(graphviz.Edge(self.node, c_node.node))\n",
    "\n",
    "def build_tree(root_string, suffixes, prefixes):\n",
    "    graph = graphviz.Dot()\n",
    "    root = Node(root_string, len(suffixes), suffixes, graph, reverse=False, highlight=True)\n",
    "    root.generate_children(prefixes, graph, True, 3)\n",
    "    return Image(graph.create_png())\n",
    "\n",
    "def get_end(string, sub_string, reverse):\n",
    "    side = 0 if reverse else -1\n",
    "    return [x for x in re.split(r'[^\\w]+', string.lower().split(sub_string)[side]) if len(x) > 0]\n",
    "\n",
    "def select_text(phrase):\n",
    "    series = questions['Title']\n",
    "    instances = series[series.str.lower().str.contains(phrase)]\n",
    "    suffixes = instances.apply(lambda x: get_end(x, phrase, False))\n",
    "    prefixes = instances.apply(lambda x: get_end(x, phrase, True))\n",
    "    return build_tree(phrase, suffixes, prefixes)\n",
    "\n",
    "select_text('file using python')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use our word tree function to explore how any phrase is used in question titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "select_text('tableau')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<div id=\"place\"></div>\n",
    "## Plot Places\n",
    "*[Table of Contents](#contents)*\n",
    "\n",
    "Where do people say they are from?\n",
    "\n",
    "In this example we'll look at how to make maps in python and why making maps is so hard.\n",
    "\n",
    "Let's start by adding information on each location from the [Google Maps API](https://developers.google.com/maps/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "location_data = pd.read_csv('loc_data.csv')\n",
    "location_data['Google_Data'] = location_data['Google_Data'].apply(lambda x: eval(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "location_data['Google_Data'].apply(len).hist()\n",
    "plt.xlabel('Number of Matches')\n",
    "plt.ylabel('Number of Unique Place Strings')\n",
    "plt.show()\n",
    "\n",
    "location_data = location_data[location_data['Google_Data'].apply(len) == 1]\n",
    "posts_with_location = pd.merge(posts, location_data, how='inner', on='Location')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How precise is our location information?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lowest_component(address_list):\n",
    "    components = address_list[0]['address_components']\n",
    "    all_parts = []\n",
    "    for c in components:\n",
    "        if len(c['types']) == 2 and c['types'][1] == 'political':\n",
    "            all_parts.append(c['types'][0])\n",
    "    if len(all_parts) > 0:\n",
    "        return all_parts[0]\n",
    "    return None\n",
    "\n",
    "posts_with_location = pd.merge(posts, location_data, how='inner', on='Location')\n",
    "lowest_components = posts_with_location['Google_Data'].apply(get_lowest_component).value_counts()\n",
    "lowest_components.plot(kind='bar', color='DarkBlue')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_part(address_list, level='country', name_type='long_name'):\n",
    "    components = address_list[0]['address_components']\n",
    "    for c in components:\n",
    "        if c['types'] == [level, 'political']:\n",
    "            return c[name_type]\n",
    "    return None\n",
    "\n",
    "for part in lowest_components.index:\n",
    "    print(part)\n",
    "    posts_with_location[part] = posts_with_location['Google_Data'].apply(lambda x: get_part(x, level=part))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "One way to plot location is to just make a convex hull around each set of latitude and longitude points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lat_long(address_list):\n",
    "    location = address_list[0]['geometry']['location']\n",
    "    return location['lat'], location['lng']\n",
    "\n",
    "posts_with_location['lat_lng'] = posts_with_location['Google_Data'].apply(get_lat_long)\n",
    "posts_with_location['lat'] = posts_with_location['lat_lng'].apply(lambda x: x[0])\n",
    "posts_with_location['lng'] = posts_with_location['lat_lng'].apply(lambda x: x[1])\n",
    "msno.geoplot(posts_with_location, x='lng', y='lat', by='country')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make a better map.\n",
    "\n",
    "In order to make a better map we need shape files which contain information on the boundries of each area we are interested in. We also need to link our data to these shape files which is usually far from trivial. In this example we will plot US counties because shape files on US states and counties are easy to find.\n",
    "\n",
    "I don't go over map projections here, but the projection of a map can be changed fairly easily. Shape files contain border information in latitude and longitude typically. You can write a projection function which can be applied to each cordinate before the map is plotted to capture the fact that the world is not square."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "county_posts = posts_with_location.loc[posts_with_location['country'] == 'United States',:]\n",
    "county_posts = county_posts.groupby(['administrative_area_level_1', 'administrative_area_level_2'])\n",
    "\n",
    "def get_lang_count(series, lang='python'):\n",
    "    return len(series[series.notnull() & (series.str.find('<'+lang+'>') > -1)])\n",
    "\n",
    "county_stats = county_posts.agg({'lat':np.mean, 'lng':np.mean, 'PostId':len, 'Tags':get_lang_count,\n",
    "                                'PostType':lambda x : len(x[x=='Question'])})\n",
    "\n",
    "county_stats.reset_index(inplace=True)\n",
    "county_stats = county_stats.rename(columns={'PostId':'posts', 'Tags':'python questions', 'PostType':'questions', 'administrative_area_level_1':'State', 'administrative_area_level_2':'County'})\n",
    "county_stats.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get shape date for counties and states\n",
    "counties = {\n",
    "    code: county for code, county in counties.items()\n",
    "}\n",
    "\n",
    "us_states = us_states_data.data.copy()\n",
    "\n",
    "name_to_code = dict([(counties[code]['detailed name'], code) for code in counties])\n",
    "\n",
    "def match_county(county):\n",
    "    state = county['State']\n",
    "    county_name = county['County']\n",
    "    # take out non-ascii characters which are not in Bokeh file\n",
    "    county_name = unicodedata.normalize('NFKD', county_name).encode('ascii','ignore').decode(\"utf-8\")\n",
    "    full_name = county_name + ', ' + state\n",
    "    if full_name in name_to_code:\n",
    "        return name_to_code[full_name]\n",
    "    close_matches = [n for n in name_to_code.keys() if n.endswith(state) and n.startswith(county_name.split(' ')[0])]\n",
    "    if len(close_matches) == 0:\n",
    "        print(full_name)\n",
    "        return None\n",
    "    full_name = min(close_matches, key=len)\n",
    "    return name_to_code[full_name]\n",
    "\n",
    "county_stats['code'] = pd.Series(county_stats.apply(match_county, axis=1))\n",
    "county_stats.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_map(county_stats, county_slice=None, language='python'):\n",
    "    color_mapper = LogColorMapper(palette=palette)\n",
    "    \n",
    "    if county_slice is not None:\n",
    "        county_stats = county_stats[county_slice]\n",
    "\n",
    "    county_xs = county_stats['code'].apply(lambda code: counties[code][\"lons\"]).tolist()\n",
    "    county_ys = county_stats['code'].apply(lambda code: counties[code][\"lats\"]).tolist()\n",
    "    county_names = (county_stats[\"County\"]+', '+county_stats[\"State\"]).tolist()\n",
    "    \n",
    "    language_perc = county_posts['Tags'].agg(lambda x: get_lang_count(x, language))\n",
    "    language_perc = language_perc.reset_index()\n",
    "    if county_slice is not None:\n",
    "        language_perc = language_perc[county_slice]\n",
    "    language_perc = (language_perc['Tags']/county_stats['questions'])\n",
    "    language_perc = (language_perc*100).tolist()\n",
    "\n",
    "    posts_source = ColumnDataSource(data=dict(\n",
    "        x=county_xs,\n",
    "        y=county_ys,\n",
    "        name=county_names,\n",
    "        posts=county_stats['posts'].tolist(),\n",
    "        questions=county_stats['questions'].tolist(),\n",
    "        lang_posts=language_perc\n",
    "    ))\n",
    "    \n",
    "    TOOLS = \"pan,wheel_zoom,reset,save\"\n",
    "\n",
    "    p = figure(\n",
    "        title=\"Posts by County\", tools=TOOLS,\n",
    "        x_axis_location=None, y_axis_location=None,\n",
    "        plot_width=900\n",
    "    )\n",
    "    p.grid.grid_line_color = None\n",
    "\n",
    "    county_pathches = Patches(xs=\"x\", ys=\"y\",\n",
    "              fill_color={'field': 'lang_posts', 'transform': color_mapper},\n",
    "              fill_alpha=0.7, line_color=\"white\", line_width=0.5)\n",
    "    county_pathches_render = p.add_glyph(posts_source, county_pathches)\n",
    "    \n",
    "    # add hover tooltip\n",
    "    hover = HoverTool(renderers=[county_pathches_render], tooltips=[\n",
    "        (\"Name\", \"@name\"),\n",
    "        (\"Posts\", \"@posts\"),\n",
    "        (\"Questions\", \"@questions\"),\n",
    "        (\"% \"+language.capitalize(), \"@lang_posts\")])\n",
    "    p.add_tools(hover)\n",
    "    \n",
    "    # -----------\n",
    "    # Add state outlines\n",
    "    # -----------\n",
    "    filter_fun = lambda x : x != 'AK' and x != 'HI'\n",
    "    # get lat and long as x and y\n",
    "    state_xs = [us_states[code][\"lons\"] for code in us_states if filter_fun(code)]\n",
    "    state_ys = [us_states[code][\"lats\"] for code in us_states if filter_fun(code)]\n",
    "    \n",
    "    # draw state lines\n",
    "    p.patches(state_xs, state_ys, fill_alpha=0.0, line_color=\"#\"+('9'*6), line_width=0.5)\n",
    "\n",
    "    show(p)\n",
    "\n",
    "build_map(county_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "build_map(county_stats, county_stats['posts'] > 30, language='python')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"network\"></div>\n",
    "## Plot Connections\n",
    "*[Table of Contents](#contents)*\n",
    "\n",
    "We can model user interactions as a graph by making an edge from each person who answers a question to the person who posted that question. We can then use graphviz to visualize this graph.\n",
    "\n",
    "To start with, we need to compute the edges between non-null users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter out null users and get question ids\n",
    "from_edges = answers.loc[answers['UserId'].notnull(),['UserId', 'QuestionId']]\n",
    "from_edges.rename(columns={'UserId':'AnswerUID', 'QuestionId':'PostId'}, inplace=True)\n",
    "# filter out null users and get question ids\n",
    "to_edges = questions.loc[questions['UserId'].notnull(),['UserId','PostId']]\n",
    "# merge on question id\n",
    "edges = pd.merge(from_edges, to_edges, on='PostId', how='inner')\n",
    "# use a counter to merge duplicate edges to get edge weights\n",
    "edges = Counter(edges.apply(lambda x:(x['AnswerUID'], x['UserId']), axis=1).tolist())\n",
    "edges.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use user reputation to color the nodes in our graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repuatation_map = dict(zip(posts['UserId'], posts['Reputation']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make some functions to visualize our graph.\n",
    "\n",
    "Graphviz has a few algorithms for deciding where nodes go. You can choose between them use the `prog` attribute when you create the graph image. The default option is dot, which works well for trees, but makes less sense for visualizing social networks. The full list of options is:\n",
    "* dot - \"hierarchical\" or layered drawings of directed graphs. This is the default tool to use if edges have directionality.\n",
    "* neato - \"spring model'' layouts.  This is the default tool to use if the graph is not too large (about 100 nodes) and you don't know anything else about it. Neato attempts to minimize a global energy function, which is equivalent to statistical multi-dimensional scaling.\n",
    "* fdp - \"spring model'' layouts similar to those of neato, but does this by reducing forces rather than working with energy.\n",
    "* twopi - radial layouts, after Graham Wills 97. Nodes are placed on concentric circles depending their distance from a given root node.\n",
    "* circo - circular layout, after Six and Tollis 99, Kauffman and Wiese 02. This is suitable for certain diagrams of multiple cyclic structures, such as certain telecommunications networks.\n",
    "\n",
    "In this example, nodes don't have labels and are filled according to a user's repuatation. If you would like to label nodes with the user's id, just change `label=''` to `label=uid` in `make_node`. In this example darker colors signify user's with less reputation. To change this just add `val = 255 - val` in `get_fill`. I like magenta, but if you want a different overall color to the graph, change the return statement for `get_fill`. These are some simple options:\n",
    "* gray = [val, val, val]\n",
    "* red = [val, 0, 0]\n",
    "* green = [0, val, 0]\n",
    "* blue = [0, 0, val]\n",
    "* yellow = [val, val, 0]\n",
    "* cyan = [0, val, val]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a function to get a fill color based on reputation\n",
    "def get_fill(uid):\n",
    "    rep = repuatation_map[int(uid)]\n",
    "    # The distribution of reputations is very lop-sided, so let's use log reputation for our scale\n",
    "    # the value should be between 0 and 255\n",
    "    val = np.log(rep)/np.log(max(repuatation_map.values())) * 255\n",
    "    return to_hex([val, 0, val])\n",
    "\n",
    "# turn a RGB triplet into a hex color graphviz will understand\n",
    "def to_hex(triple):\n",
    "    output = '#'\n",
    "    for val in triple:\n",
    "        # the hex funciton returns a string of the form 0x<number in hex>\n",
    "        val = hex(int(val)).split('x')[1]\n",
    "        if len(val) < 2:\n",
    "            val = '0'+val\n",
    "        output += val\n",
    "    return output\n",
    "\n",
    "# The function to visualize our network graph\n",
    "# It takes in a list of edges with weights\n",
    "def build_network(edges_with_weights, prog='neato'):\n",
    "    # The function which builds each node. You can change the node style here.\n",
    "    make_node = lambda uid: graphviz.Node(uid, label='', shape='circle', style='filled', fillcolor=get_fill(uid))\n",
    "    graph = graphviz.Dot()\n",
    "    # A dictionary to keep track of node objects\n",
    "    nodes = {}\n",
    "    for pair in edges_with_weights:\n",
    "        e, w = pair\n",
    "        e = (str(int(e[0])), str(int(e[1])))\n",
    "        # Add notes to the graph if they don't exist yet\n",
    "        if e[0] not in nodes:\n",
    "            nodes[e[0]] = make_node(e[0])\n",
    "            graph.add_node(nodes[e[0]])\n",
    "        if e[1] not in nodes:\n",
    "            nodes[e[1]] = make_node(e[1])\n",
    "            graph.add_node(nodes[e[1]])\n",
    "        graph.add_edge(graphviz.Edge(nodes[e[0]], nodes[e[1]], penwidth=(float(w)/2)))\n",
    "    return Image(graph.create_png(prog=prog))\n",
    "\n",
    "# Let's build a small network from the edges with the highest weights.\n",
    "build_network(edges.most_common(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from the sample of edges above, this may not be a connected graph. Let's pick a sample of edges and plot a connected subgraph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_connected_subgraphs(edges):\n",
    "    nodes = list(set([n for e in edges for n in e]))\n",
    "    mappings = dict(zip(nodes, range(len(nodes))))\n",
    "    flipped_mappings = dict(zip(range(len(nodes)), [[n] for n in nodes]))\n",
    "    for e in edges:\n",
    "        c_1 = mappings[e[0]]\n",
    "        c_2 = mappings[e[1]]\n",
    "        if c_1 == c_2:\n",
    "            continue\n",
    "        if len(flipped_mappings[c_1]) > len(flipped_mappings[c_2]):\n",
    "            tmp = c_1\n",
    "            c_1 = c_2\n",
    "            c_2 = tmp\n",
    "        for n in flipped_mappings[c_1]:\n",
    "            mappings[n] = c_2\n",
    "            flipped_mappings[c_2].append(n)\n",
    "    return mappings\n",
    "\n",
    "num_edges = 10000\n",
    "connection_mapping = find_connected_subgraphs([x[0] for x in edges.most_common(num_edges)])\n",
    "Counter(connection_mapping.values()).most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "100 nodes seems like a large enough graph.\n",
    "\n",
    "*Note: If you plot a graph with over 500 nodes you might overwhelm graphviz*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e_list = [x for x in edges.most_common(num_edges) if connection_mapping.get(x[0][0],None) == 3586]\n",
    "build_network(e_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"conclusion\"></div>\n",
    "## Conclusion\n",
    "*[Table of Contents](#contents)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
