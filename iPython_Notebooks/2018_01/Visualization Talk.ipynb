{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing Stack Overflow Data in Python\n",
    "\n",
    "In this notebook, we visualize posts on Stack Overflow from September 2017. The data was compiled from searches on the [Stack Exchange Data Explorer](https://data.stackexchange.com/stackoverflow/query/new). Location information was added using the [Google Maps API](https://developers.google.com/maps/).\n",
    "\n",
    "<div id=\"contents\"></div>\n",
    "## Table of Contents\n",
    "1. [Load the Data](#load)\n",
    "1. [Visualize Completeness](#completeness)\n",
    "1. [Visualize Time](#time)\n",
    "1. [Visualize Tags](#tags)\n",
    "1. [Explore Text](#explore)\n",
    "1. [Plot Place](#place)\n",
    "1. [Plot Connections](#network)\n",
    "1. [Conclusion](#conclusion)\n",
    "\n",
    "Make sure to go through the first two sections (data and completeness) first. The other sections can be done out of order.\n",
    "\n",
    "## Load Libraries\n",
    "This cell contains all the libraries which are necessary for this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General purpose libraries\n",
    "# A nice library for reading in csv data\n",
    "import pandas as pd\n",
    "# A library which most visualization libraries in Python are built on.\n",
    "# We will start by using it to make some plots with pandas\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "# A library for doing math\n",
    "import numpy as np\n",
    "# A library for turning unicode fields into ASCII fields\n",
    "import unicodedata\n",
    "# a regex library\n",
    "import re\n",
    "# a class which makes counting the number of times something occurs in a list easier\n",
    "from collections import Counter\n",
    "\n",
    "# some functions for displaying html in a notebook\n",
    "from IPython.core.display import display, HTML\n",
    "\n",
    "# A library to visualize holes in a dataset\n",
    "import missingno as msno\n",
    "# A library to make wordclouds\n",
    "import wordcloud\n",
    "\n",
    "# Libraries for Word Trees\n",
    "# lets us use graphviz in python\n",
    "from pydotplus import graphviz\n",
    "# to display the final Image\n",
    "from IPython.display import Image\n",
    "\n",
    "## -- Bokeh -- ##\n",
    "# Loading Bokeh\n",
    "import bokeh\n",
    "# Libraries interactive charts\n",
    "from bokeh.io import output_notebook\n",
    "# display interactive charts inline\n",
    "output_notebook()\n",
    "from bokeh.palettes import Viridis6 as palette\n",
    "from bokeh.plotting import figure, show\n",
    "from bokeh.models import HoverTool, ColorBar, LinearColorMapper, FixedTicker, ColumnDataSource, LogColorMapper\n",
    "# to make patches into glyphs and treat counties and states differently\n",
    "from bokeh.models.glyphs import Patches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following import is a pair of shape files we use for making maps. If you haven't downloaded these shape files previously, you will need to run `bokeh.sampledata.download()`. If you don't want to download these files, you will miss out on part of the mapping section, but otherwise the notebook will be unaffected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Downloading `sampledata` from Bokeh\n",
    "try:\n",
    "    import bokeh.sampledata\n",
    "except:\n",
    "    import bokeh\n",
    "    ## Downloading sample data\n",
    "    bokeh.sampledata.download()\n",
    "\n",
    "# shape files for US counties\n",
    "from bokeh.sampledata.us_counties import data as counties\n",
    "# shape files for US states\n",
    "from bokeh.sampledata import us_states as us_states_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"load\"></div>\n",
    "## Load the Data\n",
    "*[Table of Contents](#contents)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data\n",
    "posts = pd.read_csv('SeptemberPosts_reduced.csv.csv')\n",
    "posts.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Columns\n",
    "What do all these columns mean?\n",
    "1. PostId = the id in Stack Overflow's database of this post\n",
    "1. Score = the score given to the post by people voting up and down on it\n",
    "1. PostType = What type of post is this?\n",
    "1. CreationData = When was this post posted?\n",
    "1. Title = The text in the title of the post\n",
    "1. UserId = The id of the user who posted in the Stack Overflow database\n",
    "1. Reputation = The reputaiton of the user who posted\n",
    "1. Location = The location the user put down as their home on their profile\n",
    "1. Tags = Tags which are associated with this post\n",
    "1. QuestionId = The question this post is linked to\n",
    "\n",
    "Let us convert CreationDate to a datetime type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "posts['CreationDate'] = pd.to_datetime(posts['CreationDate'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"completeness\"></div>\n",
    "## Visualizing Completeness\n",
    "*[Table of Contents](#contents)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'd like to know how complete our data is, so let's look at which fields have null values for the answers and questions using [missingno](https://github.com/ResidentMario/missingno).\n",
    "\n",
    "Black indicates that the data is present while white indicates that it is missing. The column on the far right is meant to show a chart of how many variables each data point has. It is a bit hard to see on a dataset this large, but if you change the command to `msno.matrix(posts.sample(100))` it will make more sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "msno.matrix(posts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is there any correlation between when columns are null? Why don't we have a single row which is complete in all 10 columns?\n",
    "\n",
    "Missingno has a lot of ways of visualizing data completeness. One of them is to see the correlation in missing-ness between fields. It does this by generating a heatmap where blue indicates two fields tend to go missing together and red indicates two fields tend to be mutually exclusive. Fields which are always present are excluded from the heatmap, and correlations below 0.1 are grayed out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "msno.heatmap(posts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that posts with title information always have tag information and never have a question id.\n",
    "\n",
    "I wonder if these groups make up different types of posts. Let's investigate which post types we have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "post_type_counts = posts['PostType'].value_counts()\n",
    "post_type_counts.plot(kind='bar', color='DarkBlue')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is there a way to make this chart interactive?\n",
    "\n",
    "Let's use [Bokeh](https://bokeh.pydata.org/en/latest/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TOOLS = \"pan,wheel_zoom,reset,hover,save\"\n",
    "\n",
    "p = figure(\n",
    "    title=\"Post Types\",\n",
    "    tools=TOOLS,\n",
    "    x_range=post_type_counts.index.tolist(),\n",
    "    plot_height=400\n",
    ")\n",
    "p.vbar(x=post_type_counts.index.values, top=post_type_counts.values, width=0.9)\n",
    "\n",
    "hover = p.select_one(HoverTool)\n",
    "hover.point_policy = \"follow_mouse\"\n",
    "hover.tooltips = [(\"Number of Posts\", \"@top\")]\n",
    "\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most posts are either questions or answers. These types of posts serve very different purposes, so let's seperate them out and see how complete each is.\n",
    "\n",
    "This time we'll try a different method of visualizing missing data in which we count up how often each attribute is not missing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "questions = posts[posts['PostType'] == 'Question']\n",
    "answers = posts[posts['PostType'] == 'Answer']\n",
    "\n",
    "print(\"Questions\")\n",
    "msno.bar(questions)\n",
    "print(\"Answers\")\n",
    "msno.bar(answers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that UserId is missing for 1,171 questions and 785 answers. This is only .4% of the data overall, but it seems strange that a post can exist without a user to make it.\n",
    "\n",
    "Usually, when you fing out that you having missing data you want to know why and what is going on with those points. Fortunately Pandas makes this very easy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "posts[posts['UserId'].isnull()].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to investigate these posts further, we can see posts and answers with null `UserId`s using Jupyter's HTML capabilities. The following code creates links we can click on to see each post on Stack Overflow. Compare the user descriptions for the linked posts to the user descriptions for other posts on the webpage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_link(p, desc='has no UserId'):\n",
    "    if p['PostType'] == 'Answer':\n",
    "        link = '\"https://stackoverflow.com/questions/{0}#answer-{1}\"'.format(int(p['QuestionId']), int(p['PostId']))\n",
    "        return '<a href='+link+' target=\"_blank\">Answer {0} {1}</a>'.format(int(p['PostId']), desc)\n",
    "    else:\n",
    "        link = '\"https://stackoverflow.com/questions/{0}\"'.format(int(p['PostId']))\n",
    "        return '<a href='+link+' target=\"_blank\">Question {0} {1}</a>'.format(int(p['PostId']), desc)\n",
    "\n",
    "display(HTML('<br/>'.join(posts[posts['UserId'].isnull()].head().apply(lambda p: get_link(p), axis=1))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Did you notice that unlike other user descriptions, the users of the linked posts had no link to their profile page and no information about their reputation. My theory is that these users deleted their accounts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"time\"></div>\n",
    "## Visualizing Time\n",
    "*[Table of Contents](#contents)*\n",
    "\n",
    "1. What time of day do people post?\n",
    "1. [How quickly were questions answered?](#firstReply)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "posts['CreationDate'].apply(lambda x: x.hour).hist(bins=range(24))\n",
    "plt.xlabel('Hour of day on a 24 hour clock')\n",
    "plt.ylabel('Number of posts')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do you see anything odd in the distribution of when posts are created?\n",
    "\n",
    "<div id=\"firstReply\"></div>\n",
    "How quickly were questions answered?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# aggregate answers by question id\n",
    "answers_by_question = answers.groupby('QuestionId')['CreationDate'].agg(min)\n",
    "# get the earliest creation date for each answer\n",
    "first_reply = pd.DataFrame({'PostId':answers_by_question.index.values, 'EarliestReply':answers_by_question.values})\n",
    "# add the time of the earliest answer to the questions data frame (filtering out questions which were not answered)\n",
    "first_reply = pd.merge(first_reply, questions, how='inner', on=['PostId'])\n",
    "\n",
    "# get the time it took to get an answer\n",
    "gap = (first_reply['EarliestReply']-first_reply['CreationDate'])\n",
    "# convert to minutes\n",
    "gap /= pd.Timedelta(minutes=1)\n",
    "\n",
    "# find the median\n",
    "print('Median answer time for questions asked and answered in September 2017 is {0} min.'.format(gap.median()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get a better idea of the distribution of question to answer latency, let's plot a histogram of this data. Since this distribution probably has a long tail, let's use a log scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's plot the data\n",
    "plt.hist(gap.tolist(), bins = 50)\n",
    "plt.yscale('log', nonposy='clip')\n",
    "plt.ylabel('Number of Questions')\n",
    "plt.xlabel('Time in Minutes')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do you notice a particularly odd value in this chart?\n",
    "\n",
    "How can we have an answer before the question was asked?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weird_questions = first_reply[first_reply['EarliestReply'] < first_reply['CreationDate']]\n",
    "links = weird_questions.apply(lambda p: get_link(p, desc='answered before question'), axis=1)\n",
    "display(HTML('<br/>'.join(links)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"tags\"></div>\n",
    "## Visualize Tags\n",
    "*[Table of Contents](#contents)*\n",
    "\n",
    "How can we use wordclouds to explore tags as function of title keywords?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def text_to_wordcloud(series, title):\n",
    "    # stitch all the text together\n",
    "    text = ' '.join(series.tolist())\n",
    "    # make a wordcloud from the text\n",
    "    title_wordcloud = wordcloud.WordCloud().generate(text)\n",
    "    # we want the words in our cloud to all be the same color\n",
    "    title_wordcloud.recolor(color_func=lambda word, **kwargs:'white')\n",
    "    # turn the wordcloud into an image\n",
    "    plt.imshow(title_wordcloud, interpolation='bilinear')\n",
    "    # we don't want an x and y axis\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(title + ' (' + str(len(series)) + ' questions)')\n",
    "    plt.show()\n",
    "\n",
    "def get_tags(word):\n",
    "    tags = questions[questions['Title'].str.lower().str.contains(word)]['Tags']\n",
    "    tags = tags.apply(lambda x: x.strip('<>').split('><'))\n",
    "    return tags\n",
    "\n",
    "keyword_1 = 'data'\n",
    "keyword_2 = 'fast'\n",
    "\n",
    "group_1_tags = get_tags(keyword_1)\n",
    "text_to_wordcloud(group_1_tags.apply(lambda x: ' '.join(x)), \"Tags when '\"+keyword_1+\"' in question title\")\n",
    "group_2_tags = get_tags(keyword_2)\n",
    "text_to_wordcloud(group_2_tags.apply(lambda x: ' '.join(x)), \"Tags when '\"+keyword_2+\"' in question title\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of using word clouds, let's try plotting the number of times a tag appears using a more traditional chart type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_tag_count(series):\n",
    "    return pd.Series([t for tag_list in series for t in tag_list]).value_counts()\n",
    "\n",
    "tag_counts = pd.concat([get_tag_count(group_1_tags), get_tag_count(group_2_tags)], axis=1)\n",
    "tag_counts = tag_counts.rename(columns={0:keyword_1, 1:keyword_2})\n",
    "tag_counts = tag_counts[tag_counts[keyword_1].notnull() & tag_counts[keyword_2].notnull()]\n",
    "\n",
    "x = tag_counts[keyword_1].values\n",
    "y = tag_counts[keyword_2].values\n",
    "TOOLS = \"pan,wheel_zoom,reset,hover,save\"\n",
    "\n",
    "source = ColumnDataSource(data=dict(\n",
    "    x=x,\n",
    "    y=y,\n",
    "    group_1_perc=[round(val/len(group_1_tags)*100,2) for val in x],\n",
    "    group_2_perc=[round(val/len(group_2_tags)*100,2) for val in y],\n",
    "    name=tag_counts.index.tolist()\n",
    "))\n",
    "\n",
    "# uncomment the lines to switch this to a log scale to see the number for less popular languages\n",
    "p = figure(\n",
    "    title=\"Tags\",\n",
    "    tools=TOOLS,\n",
    "#     x_axis_type=\"log\",\n",
    "#     y_axis_type=\"log\",\n",
    "    plot_height=400\n",
    ")\n",
    "p.circle('x', 'y', size=10, source=source)\n",
    "p.xaxis.axis_label = \"'\"+keyword_1+\"' in question title\"\n",
    "p.yaxis.axis_label = \"'\"+keyword_2+\"' in question title\"\n",
    "\n",
    "hover = p.select_one(HoverTool)\n",
    "hover.point_policy = \"follow_mouse\"\n",
    "hover.tooltips = [(\"Tag\", \"@name\"), (\"% of '\"+keyword_1+\"' questions:\", \"@group_1_perc\"), (\"% of '\"+keyword_2+\"' questions:\", \"@group_2_perc\")]\n",
    "\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to this analysis what tag do people use when they are interested in looking at data quickly? Can you think of some other keywords that would be fun to plot against each other?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"explore\"></div>\n",
    "## Text Visualization\n",
    "*[Table of Contents](#contents)*\n",
    "\n",
    "What is in the title of questions tagged 'python'?\n",
    "\n",
    "One way of exploring this question is to use a word cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "python_titles = questions[questions['Tags'].str.contains('python')]['Title'].str.lower()\n",
    "text_to_wordcloud(python_titles, \"'Python' Tagged Questions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other Methods of Looking at Word Frequencies\n",
    "\n",
    "One problem with word clouds is that they divorce the words in them from any context. This isn't much of a problem when the words are tags, which aren't part of a sentence. But when looking at titles it would be nice to see what users are saying when they use the words 'python', 'using' and 'file'.\n",
    "\n",
    "To really understand all 19,462 questions tagged 'python', we'd have to break out some machine learning methods, but their are ways visualize context better than a word cloud. One of them is called a [Word Tree](http://hint.fm/papers/wordtree_final2.pdf) and was developed by the many eyes group at IBM. You can see [examples](https://www.jasondavies.com/wordtree/) of this style of visualization made in d3.js.\n",
    "\n",
    "Since Jupyter notebooks can embed HTML elements, word trees rendered in d3 can be embedded in a notebook. However, for this tutorial we are sticking to python, so I will demonstrate how to build a word tree using a library which runs [graphviz](http://graphviz.org) in python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# a variable to help us mark nodes as distinct when they have the same label\n",
    "node_counter = 0\n",
    "\n",
    "# a class to keep track of a node and it's connections\n",
    "class Node:\n",
    "    def __init__(self, word, count, matching_strings, graph, reverse=False, branching=3, highlight=False):\n",
    "        global node_counter\n",
    "        if highlight:\n",
    "            self.node = graphviz.Node(node_counter, label=word+'\\n'+str(count), peripheries=2, fontsize=20)\n",
    "        else:\n",
    "            self.node = graphviz.Node(node_counter, label=word+'\\n'+str(count))\n",
    "        node_counter += 1\n",
    "        graph.add_node(self.node)\n",
    "        if count > 1:\n",
    "            self.generate_children(matching_strings, graph, reverse, branching)\n",
    "    \n",
    "    def generate_children(self, matching_strings, graph, reverse, branching):\n",
    "        if len(matching_strings) == 0:\n",
    "            return\n",
    "        matching_strings = matching_strings[matching_strings.apply(len) > 0]\n",
    "        all_children = Counter(matching_strings.apply(lambda x:x[-1 if reverse else 0]))\n",
    "        children = all_children.most_common(branching)\n",
    "        for word, count in children:\n",
    "            if not reverse:\n",
    "                child_matches = matching_strings[matching_strings.apply(lambda x:x[0]) == word].apply(lambda x:x[1:])\n",
    "                c_node = Node(word, count, child_matches, graph=graph, reverse=reverse, branching=branching)\n",
    "                graph.add_edge(graphviz.Edge(self.node, c_node.node))\n",
    "            else:\n",
    "                child_matches = matching_strings[matching_strings.apply(lambda x:x[-1]) == word].apply(lambda x:x[:-1])\n",
    "                c_node = Node(word, count, child_matches, graph=graph, reverse=reverse, branching=branching)\n",
    "                graph.add_edge(graphviz.Edge(c_node.node, self.node))\n",
    "        left_over = sum(all_children.values()) - sum([x[1] for x in children])\n",
    "        if left_over > 0:\n",
    "            c_node = Node('...', left_over, [], graph=graph, reverse=reverse, branching=branching)\n",
    "            if reverse:\n",
    "                graph.add_edge(graphviz.Edge(c_node.node, self.node))\n",
    "            else:\n",
    "                graph.add_edge(graphviz.Edge(self.node, c_node.node))\n",
    "\n",
    "def build_tree(root_string, suffixes, prefixes):\n",
    "    graph = graphviz.Dot()\n",
    "    root = Node(root_string, len(suffixes), suffixes, graph, reverse=False, highlight=True)\n",
    "    root.generate_children(prefixes, graph, True, 3)\n",
    "    return Image(graph.create_png())\n",
    "\n",
    "def get_end(string, sub_string, reverse):\n",
    "    side = 0 if reverse else -1\n",
    "    return [x for x in re.split(r'[^\\w]+', string.lower().split(sub_string)[side]) if len(x) > 0]\n",
    "\n",
    "def select_text(phrase):\n",
    "    series = questions['Title']\n",
    "    instances = series[series.str.lower().str.contains(phrase)]\n",
    "    suffixes = instances.apply(lambda x: get_end(x, phrase, False))\n",
    "    prefixes = instances.apply(lambda x: get_end(x, phrase, True))\n",
    "    return build_tree(phrase, suffixes, prefixes)\n",
    "\n",
    "select_text('file using python')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use our word tree function to explore how any phrase is used in question titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "select_text('tableau')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<div id=\"place\"></div>\n",
    "## Plot Places\n",
    "*[Table of Contents](#contents)*\n",
    "\n",
    "Where do people say they are from?\n",
    "\n",
    "In this example we'll look at how to make maps in python and why making maps is so hard.\n",
    "\n",
    "Let's start by adding information on each location from the [Google Maps API](https://developers.google.com/maps/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "location_data = pd.read_csv('loc_data_reduced.csv')\n",
    "location_data['Google_Data'] = location_data['Google_Data'].apply(lambda x: eval(x))\n",
    "\n",
    "location_data['Google_Data'].apply(len).hist()\n",
    "#plt.yscale('log', nonposy='clip')\n",
    "plt.xlabel('Number of Matches')\n",
    "plt.ylabel('Number of Unique Place Strings')\n",
    "plt.show()\n",
    "\n",
    "location_data = location_data[location_data['Google_Data'].apply(len) == 1]\n",
    "posts_with_location = pd.merge(posts, location_data, how='inner', on='Location')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How precise is our location information?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_lowest_component(address_list):\n",
    "    components = address_list[0]['address_components']\n",
    "    all_parts = []\n",
    "    for c in components:\n",
    "        if len(c['types']) == 2 and c['types'][1] == 'political':\n",
    "            all_parts.append(c['types'][0])\n",
    "    if len(all_parts) > 0:\n",
    "        return all_parts[0]\n",
    "    return None\n",
    "\n",
    "posts_with_location = pd.merge(posts, location_data, how='inner', on='Location')\n",
    "lowest_components = posts_with_location['Google_Data'].apply(get_lowest_component).value_counts()\n",
    "lowest_components.plot(kind='bar', color='DarkBlue')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's add information on each level of location we have available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_part(address_list, level='country', name_type='long_name'):\n",
    "    components = address_list[0]['address_components']\n",
    "    for c in components:\n",
    "        if c['types'] == [level, 'political']:\n",
    "            return c[name_type]\n",
    "    return None\n",
    "\n",
    "for part in lowest_components.index:\n",
    "    print(part)\n",
    "    posts_with_location[part] = posts_with_location['Google_Data'].apply(lambda x: get_part(x, level=part))\n",
    "\n",
    "posts_with_location.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "One way to plot location is to just make a convex hull around each set of latitude and longitude points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_lat_long(address_list):\n",
    "    location = address_list[0]['geometry']['location']\n",
    "    return location['lat'], location['lng']\n",
    "\n",
    "posts_with_location['lat_lng'] = posts_with_location['Google_Data'].apply(get_lat_long)\n",
    "posts_with_location['lat'] = posts_with_location['lat_lng'].apply(lambda x: x[0])\n",
    "posts_with_location['lng'] = posts_with_location['lat_lng'].apply(lambda x: x[1])\n",
    "msno.geoplot(posts_with_location, x='lng', y='lat', by='country')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make a better map.\n",
    "\n",
    "In order to make a better map we need shape files which contain information on the boundries of each area we are interested in. We also need to link our data to these shape files which is usually far from trivial. In this example we will plot US counties because shape files on US states and counties are easy to find.\n",
    "\n",
    "I don't go over map projections here, but the projection of a map can be changed fairly easily. Shape files contain border information in latitude and longitude typically. You can write a projection function which can be applied to each cordinate before the map is plotted to capture the fact that the world is not square."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "county_posts = posts_with_location.loc[posts_with_location['country'] == 'United States',:]\n",
    "county_posts = county_posts.groupby(['administrative_area_level_1', 'administrative_area_level_2'])\n",
    "\n",
    "def get_lang_count(series, lang='python'):\n",
    "    return len(series[series.notnull() & (series.str.find('<'+lang+'>') > -1)])\n",
    "\n",
    "county_stats = county_posts.agg({'lat':np.mean, 'lng':np.mean, 'PostId':len, 'Tags':get_lang_count,\n",
    "                                'PostType':lambda x : len(x[x=='Question'])})\n",
    "\n",
    "county_stats.reset_index(inplace=True)\n",
    "county_stats = county_stats.rename(columns={'PostId':'posts', 'Tags':'python questions', 'PostType':'questions', 'administrative_area_level_1':'State', 'administrative_area_level_2':'County'})\n",
    "county_stats.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get shape date for counties and states\n",
    "counties = {\n",
    "    code: county for code, county in counties.items()\n",
    "}\n",
    "\n",
    "us_states = us_states_data.data.copy()\n",
    "\n",
    "name_to_code = dict([(counties[code]['detailed name'], code) for code in counties])\n",
    "\n",
    "def match_county(county):\n",
    "    state = county['State']\n",
    "    county_name = county['County']\n",
    "    # take out non-ascii characters which are not in Bokeh file\n",
    "    county_name = unicodedata.normalize('NFKD', county_name).encode('ascii','ignore').decode(\"utf-8\")\n",
    "    full_name = county_name + ', ' + state\n",
    "    if full_name in name_to_code:\n",
    "        return name_to_code[full_name]\n",
    "    close_matches = [n for n in name_to_code.keys() if n.endswith(state) and n.startswith(county_name.split(' ')[0])]\n",
    "    if len(close_matches) == 0:\n",
    "        print(full_name)\n",
    "        return None\n",
    "    full_name = min(close_matches, key=len)\n",
    "    return name_to_code[full_name]\n",
    "\n",
    "county_stats['code'] = pd.Series(county_stats.apply(match_county, axis=1))\n",
    "county_stats.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_map(county_stats, county_slice=None, language='python'):\n",
    "    color_mapper = LogColorMapper(palette=palette)\n",
    "    \n",
    "    if county_slice is not None:\n",
    "        county_stats = county_stats[county_slice]\n",
    "\n",
    "    county_xs = county_stats['code'].apply(lambda code: counties[code][\"lons\"]).tolist()\n",
    "    county_ys = county_stats['code'].apply(lambda code: counties[code][\"lats\"]).tolist()\n",
    "    county_names = (county_stats[\"County\"]+', '+county_stats[\"State\"]).tolist()\n",
    "    \n",
    "    language_perc = county_posts['Tags'].agg(lambda x: get_lang_count(x, language))\n",
    "    language_perc = language_perc.reset_index()\n",
    "    if county_slice is not None:\n",
    "        language_perc = language_perc[county_slice]\n",
    "    language_perc = (language_perc['Tags']/county_stats['questions'])\n",
    "    language_perc = (language_perc*100).tolist()\n",
    "\n",
    "    posts_source = ColumnDataSource(data=dict(\n",
    "        x=county_xs,\n",
    "        y=county_ys,\n",
    "        name=county_names,\n",
    "        posts=county_stats['posts'].tolist(),\n",
    "        questions=county_stats['questions'].tolist(),\n",
    "        lang_posts=language_perc\n",
    "    ))\n",
    "    \n",
    "    TOOLS = \"pan,wheel_zoom,reset,save\"\n",
    "\n",
    "    p = figure(\n",
    "        title=\"Posts by County\", tools=TOOLS,\n",
    "        x_axis_location=None, y_axis_location=None,\n",
    "        plot_width=900\n",
    "    )\n",
    "    p.grid.grid_line_color = None\n",
    "\n",
    "    county_pathches = Patches(xs=\"x\", ys=\"y\",\n",
    "              fill_color={'field': 'lang_posts', 'transform': color_mapper},\n",
    "              fill_alpha=0.7, line_color=\"white\", line_width=0.5)\n",
    "    county_pathches_render = p.add_glyph(posts_source, county_pathches)\n",
    "    \n",
    "    # add hover tooltip\n",
    "    hover = HoverTool(renderers=[county_pathches_render], tooltips=[\n",
    "        (\"Name\", \"@name\"),\n",
    "        (\"Posts\", \"@posts\"),\n",
    "        (\"Questions\", \"@questions\"),\n",
    "        (\"% \"+language.capitalize(), \"@lang_posts\")])\n",
    "    p.add_tools(hover)\n",
    "    \n",
    "    # -----------\n",
    "    # Add state outlines\n",
    "    # -----------\n",
    "    filter_fun = lambda x : x != 'AK' and x != 'HI'\n",
    "    # get lat and long as x and y\n",
    "    state_xs = [us_states[code][\"lons\"] for code in us_states if filter_fun(code)]\n",
    "    state_ys = [us_states[code][\"lats\"] for code in us_states if filter_fun(code)]\n",
    "    \n",
    "    # draw state lines\n",
    "    p.patches(state_xs, state_ys, fill_alpha=0.0, line_color=\"#\"+('9'*6), line_width=0.5)\n",
    "\n",
    "    show(p)\n",
    "\n",
    "build_map(county_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "build_map(county_stats, county_stats['posts'] > 30, language='python')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"network\"></div>\n",
    "## Plot Connections\n",
    "*[Table of Contents](#contents)*\n",
    "\n",
    "We can model user interactions as a graph by making an edge from each person who answers a question to the person who posted that question. We can then use graphviz to visualize this graph.\n",
    "\n",
    "To start with, we need to compute the edges between non-null users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# filter out null users and get question ids\n",
    "from_edges = answers.loc[answers['UserId'].notnull(),['UserId', 'QuestionId']]\n",
    "from_edges.rename(columns={'UserId':'AnswerUID', 'QuestionId':'PostId'}, inplace=True)\n",
    "# filter out null users and get question ids\n",
    "to_edges = questions.loc[questions['UserId'].notnull(),['UserId','PostId']]\n",
    "# merge on question id\n",
    "links = pd.merge(from_edges, to_edges, on='PostId', how='inner')\n",
    "# use a counter to merge duplicate edges to get edge weights\n",
    "edges = Counter(links.apply(lambda x:(x['AnswerUID'], x['UserId']), axis=1).tolist())\n",
    "edges.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use user reputation to color the nodes in our graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "repuatation_map = dict(zip(posts['UserId'], posts['Reputation']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make some functions to visualize our graph.\n",
    "\n",
    "Graphviz has a few algorithms for deciding where nodes go. You can choose between them use the `prog` attribute when you create the graph image. The default option is dot, which works well for trees, but makes less sense for visualizing social networks. The full list of options is:\n",
    "* dot - \"hierarchical\" or layered drawings of directed graphs. This is the default tool to use if edges have directionality.\n",
    "* neato - \"spring model'' layouts.  This is the default tool to use if the graph is not too large (about 100 nodes) and you don't know anything else about it. Neato attempts to minimize a global energy function, which is equivalent to statistical multi-dimensional scaling.\n",
    "* fdp - \"spring model'' layouts similar to those of neato, but does this by reducing forces rather than working with energy.\n",
    "* twopi - radial layouts, after Graham Wills 97. Nodes are placed on concentric circles depending their distance from a given root node.\n",
    "* circo - circular layout, after Six and Tollis 99, Kauffman and Wiese 02. This is suitable for certain diagrams of multiple cyclic structures, such as certain telecommunications networks.\n",
    "\n",
    "In this example, nodes don't have labels and are filled according to a user's repuatation. If you would like to label nodes with the user's id, just change `label=''` to `label=uid` in `make_node`. In this example darker colors signify user's with less reputation. To change this just add `val = 255 - val` in `get_fill`. I like magenta, but if you want a different overall color to the graph, change the return statement for `get_fill`. These are some simple options:\n",
    "* gray = [val, val, val]\n",
    "* red = [val, 0, 0]\n",
    "* green = [0, val, 0]\n",
    "* blue = [0, 0, val]\n",
    "* yellow = [val, val, 0]\n",
    "* cyan = [0, val, val]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# a function to get a fill color based on reputation\n",
    "def get_fill(uid):\n",
    "    rep = repuatation_map[int(uid)]\n",
    "    # The distribution of reputations is very lop-sided, so let's use log reputation for our scale\n",
    "    # the value should be between 0 and 255\n",
    "    val = np.log(rep)/np.log(max(repuatation_map.values())) * 255\n",
    "    return to_hex([val, 0, val])\n",
    "\n",
    "# turn a RGB triplet into a hex color graphviz will understand\n",
    "def to_hex(triple):\n",
    "    output = '#'\n",
    "    for val in triple:\n",
    "        # the hex funciton returns a string of the form 0x<number in hex>\n",
    "        val = hex(int(val)).split('x')[1]\n",
    "        if len(val) < 2:\n",
    "            val = '0'+val\n",
    "        output += val\n",
    "    return output\n",
    "\n",
    "# The function to visualize our network graph\n",
    "# It takes in a list of edges with weights\n",
    "def build_network(edges_with_weights, prog='neato'):\n",
    "    # The function which builds each node. You can change the node style here.\n",
    "    make_node = lambda uid: graphviz.Node(uid, label='', shape='circle', style='filled', fillcolor=get_fill(uid), color='white')\n",
    "    graph = graphviz.Dot()\n",
    "    # A dictionary to keep track of node objects\n",
    "    nodes = {}\n",
    "    for pair in edges_with_weights:\n",
    "        e, w = pair\n",
    "        e = (str(int(e[0])), str(int(e[1])))\n",
    "        # Add notes to the graph if they don't exist yet\n",
    "        if e[0] not in nodes:\n",
    "            nodes[e[0]] = make_node(e[0])\n",
    "            graph.add_node(nodes[e[0]])\n",
    "        if e[1] not in nodes:\n",
    "            nodes[e[1]] = make_node(e[1])\n",
    "            graph.add_node(nodes[e[1]])\n",
    "        graph.add_edge(graphviz.Edge(nodes[e[0]], nodes[e[1]], penwidth=(float(w)/2)))\n",
    "    return Image(graph.create_png(prog=prog))\n",
    "\n",
    "# Let's build a small network from the edges with the highest weights.\n",
    "build_network(edges.most_common(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that a two of the ten highest weighted edges are self loops. Who are the students who answer their own questions so many times and what are the questions where they post and answer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "[x for x in edges.most_common(10) if x[0][0] == x[0][1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "uid = 4058737\n",
    "self_links = links.loc[(links['AnswerUID'] == uid) & (links['UserId'] == uid),:].copy()\n",
    "self_links['PostType'] = 'Question'\n",
    "display(HTML('<br/>'.join(self_links.apply(lambda p: get_link(p, 'is a self link'), axis=1))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from the sample of edges above, this may not be a connected graph. Let's pick a sample of edges and plot a connected subgraph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_connected_subgraphs(edges):\n",
    "    nodes = list(set([n for e in edges for n in e]))\n",
    "    mappings = dict(zip(nodes, range(len(nodes))))\n",
    "    flipped_mappings = dict(zip(range(len(nodes)), [[n] for n in nodes]))\n",
    "    for e in edges:\n",
    "        c_1 = mappings[e[0]]\n",
    "        c_2 = mappings[e[1]]\n",
    "        if c_1 == c_2:\n",
    "            continue\n",
    "        if len(flipped_mappings[c_1]) > len(flipped_mappings[c_2]):\n",
    "            tmp = c_1\n",
    "            c_1 = c_2\n",
    "            c_2 = tmp\n",
    "        for n in flipped_mappings[c_1]:\n",
    "            mappings[n] = c_2\n",
    "            flipped_mappings[c_2].append(n)\n",
    "    return mappings\n",
    "\n",
    "num_edges = 10000\n",
    "connection_mapping = find_connected_subgraphs([x[0] for x in edges.most_common(num_edges)])\n",
    "Counter(connection_mapping.values()).most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The list above gives an identification number for a connected subgraph and the number of nodes which are in that subgraph.\n",
    "\n",
    "100 nodes seems like a large enough graph.\n",
    "\n",
    "*Note: If you plot a graph with over 500 nodes you might overwhelm graphviz*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "subgraph_id_number = 3586\n",
    "e_list = [x for x in edges.most_common(num_edges) if connection_mapping.get(x[0][0],None) == subgraph_id_number]\n",
    "build_network(e_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"conclusion\"></div>\n",
    "## Conclusion\n",
    "*[Table of Contents](#contents)*\n",
    "\n",
    "Hopefully this tutorial tought you something new about visualization in python. Some key points I'd like to highlight are:\n",
    "\n",
    "1. visualize early - making visualizations early on can help a lot with data cleaning\n",
    "1. visualize for yourself first - visualization is a powerful tool for showing you what is going on in your data and if you can't understand your own visualizations, no one else will\n",
    "1. barcharts are awesome, but not everything should be a barchart - wordtrees, network diagrams and heatmaps are also nice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
